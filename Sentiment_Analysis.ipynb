{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J8CTswUIGFTl"
      },
      "outputs": [],
      "source": [
        "#Import required labraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPwo2wo7GyYC",
        "outputId": "46355ae5-078e-42a3-e1ab-f8e28b9d42d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#load the IMDB dataset\n",
        "num_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "syltoc4PHIh0"
      },
      "outputs": [],
      "source": [
        "maxlen = 200\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DXarPMVHndi",
        "outputId": "c648a6f0-b272-4a02-f198-adda8c41c17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Model building step\n",
        "model = keras.Sequential([\n",
        "    layers. Embedding(input_dim=num_words, output_dim=128, input_length=maxlen),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(1, activation='sigmoid')   #binary classification\n",
        "])#Model building step\n",
        "model = keras.Sequential([\n",
        "    layers. Embedding(input_dim=num_words, output_dim=128, input_length=maxlen),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(1, activation='sigmoid')   #binary classification\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTQR-7WmISBH",
        "outputId": "ff7bb769-8246-4464-9ee4-568fbc6fc001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 148ms/step - accuracy: 0.6881 - loss: 0.5700 - val_accuracy: 0.8094 - val_loss: 0.4214\n",
            "Epoch 2/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 137ms/step - accuracy: 0.8893 - loss: 0.2892 - val_accuracy: 0.8736 - val_loss: 0.3234\n",
            "Epoch 3/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 138ms/step - accuracy: 0.9315 - loss: 0.1870 - val_accuracy: 0.8362 - val_loss: 0.3644\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b19aa11f790>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyLe2qZiI5id",
        "outputId": "6ae4e070-63d0-419c-962c-e37fb60572a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.8349 - loss: 0.3736\n",
            "Test accuracy:  0.84\n"
          ]
        }
      ],
      "source": [
        "#Evaluate the model\n",
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {acc: .2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B78LOU2cJUwd",
        "outputId": "2fa7d836-4698-4171-f61e-348467f8f0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
            "Possitive  [[0.74322283]]\n"
          ]
        }
      ],
      "source": [
        "#Predict on new text\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  words= text.lower().split()\n",
        "  encoded = [word_index.get(w,2) for w in words]\n",
        "  return keras.preprocessing.sequence.pad_sequences([encoded], maxlen=maxlen)\n",
        "\n",
        "sample_review = \"I really love this.\"\n",
        "prediction = model.predict(encode_text(sample_review))\n",
        "print(\"Possitive \" if prediction > 0.5 else \"Negative\", prediction )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo54FyAvlbY5"
      },
      "source": [
        "2nd Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CRkUtzxOlZzt"
      },
      "outputs": [],
      "source": [
        "#Import required labraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im1T7goslapf",
        "outputId": "57e5dd70-a7b8-4bf8-adb8-f2668383a45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#load the  MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XGFmsCfup4f4"
      },
      "outputs": [],
      "source": [
        "maxlen = 200\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bOtFg_qrqAd9"
      },
      "outputs": [],
      "source": [
        "#Model building step\n",
        "model = keras.Sequential([\n",
        "    layers. Embedding(input_dim=num_words, output_dim=128, input_length=maxlen),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(1, activation='sigmoid')   #binary classification\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6pHlKxgrqnL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbXpQLqHrrLW",
        "outputId": "1a2bd77d-f583-4b96-d930-da7acacc913b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 965ms/step - accuracy: 0.8234 - loss: 0.5686 - val_accuracy: 0.9705 - val_loss: 0.0988\n",
            "Epoch 2/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 949ms/step - accuracy: 0.9755 - loss: 0.0772 - val_accuracy: 0.9848 - val_loss: 0.0527\n",
            "Epoch 3/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 955ms/step - accuracy: 0.9853 - loss: 0.0491 - val_accuracy: 0.9862 - val_loss: 0.0494\n",
            "Epoch 4/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 908ms/step - accuracy: 0.9881 - loss: 0.0346 - val_accuracy: 0.9872 - val_loss: 0.0447\n",
            "Epoch 5/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 958ms/step - accuracy: 0.9923 - loss: 0.0254 - val_accuracy: 0.9877 - val_loss: 0.0455\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - accuracy: 0.9866 - loss: 0.0405\n",
            "Test accuracy: 0.99\n"
          ]
        }
      ],
      "source": [
        "# Reshape the data to include a channel dimension\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_one_hot = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test_one_hot = keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define a CNN model for MNIST\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 200, 1)), # Input shape should be (height, width, channels) - Need to adjust padding or handle differently\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax') # 10 classes for MNIST digits\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy', # Use categorical crossentropy for multi-class classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train_one_hot, epochs=5, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, acc = model.evaluate(x_test, y_test_one_hot)\n",
        "print(f\"Test accuracy: {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7ZxSSidOtOec"
      },
      "outputs": [],
      "source": [
        "#Predict on new data\n",
        "\n",
        "# Since this model is trained on MNIST images, text prediction is not applicable.\n",
        "# The code below was for the previous IMDB model and is removed.\n",
        "\n",
        "# word_index = keras.datasets.imdb.get_word_index()\n",
        "\n",
        "# def encode_text(text):\n",
        "#   words= text.lower().split()\n",
        "#   encoded = [word_index.get(w,2) for w in words]\n",
        "#   return keras.preprocessing.sequence.pad_sequences([encoded], maxlen=maxlen)\n",
        "\n",
        "# sample_review = \"I really love this.\"\n",
        "# prediction = model.predict(encode_text(sample_review))\n",
        "# print(\"Possitive \" if prediction > 0.5 else \"Negative\", prediction )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}